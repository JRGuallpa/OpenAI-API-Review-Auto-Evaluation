## Question 1

What is the most common sentiment observed in your sample of 50 reviews according to your OpenAI labeled data?

The most common sentiment observed in the sample of 50 reviews is "negative", with a total count of 36. This indicates that the majority of customers perceive our product negatively, suggesting potential issues with quality, usability, or customer satisfaction.


## Question 2

How reliable do you believe these labels are? Look at the respective labels OpenAI has generated for specific reviews, does it seem like the large language model accurately described the user's review? What risk do model hallucinations introduce into this analysis?

The labels are somewhat reliable. After briefly skimming the reviews.json file, there appears to be a pattern of negative responses that align with GPT’s assessment. However, the risk of model hallucination includes generating inaccurate insights—such as labeling "I love this terrible coconut" as "positive" instead of "negative." Additionally, our stakeholders may lose trust in a model that cannot consistently produce accurate insights. If it fails to meet a certain threshold of reliability, it risks becoming ineffective and ultimately unusable.


## Question 3

Using the most common sentiment, what would you recommend to this Coconut Water producer to improve customer satisfaction? Should they continue to pursue current market/product outcomes, or does there exist an opportunity for this business to improve its product?


A whopping 76% of customers expressed negative sentiments about the product. Most notably, I would recommend the company review their plastic bottle distributor, which was the source of several complaints. This presents the greatest opportunity for improvement—if they fail to address this issue, the product risks quickly falling out of favor with both new and returning consumers.